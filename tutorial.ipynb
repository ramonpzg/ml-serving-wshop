{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Model Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Overview\n",
    "2. ML Serving Process\n",
    "3. Serving our first model\n",
    "4. Introduction to MLServer\n",
    "5. Serving Classic ML Models\n",
    "6. Multi-Model Serving\n",
    "7. Serving Custom Models\n",
    "9. Batch Inference\n",
    "10. Packaging\n",
    "11. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, we will delve into the intricacies of serving \n",
    "machine learning models, ensuring that both experts and beginners alike can gain \n",
    "valuable insights. We will cover the essential components, best practices, and \n",
    "practical strategies for packaging and serving.\n",
    "\n",
    "We will start by going over the machine learning lifecycle and then we will train our own \n",
    "model and showcase different ways of serving it in a step-by-step fashion.\n",
    "\n",
    "The tools we will be using are the following ones.\n",
    "\n",
    "- `scikit-learn`\n",
    "- `fastapi`\n",
    "- `mlserver`\n",
    "- `mlserver_sklearn`\n",
    "- `pydantic`\n",
    "- `joblib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the machine learning deployment lifecycle as a 5-step process \n",
    "that starts once you have collected data, trained and evaluated a model. Here are \n",
    "the steps.\n",
    "\n",
    "\n",
    "1. Serialize and Package the Model:\n",
    "   - Serialize the trained model into a format suitable for deployment (e.g., pickle, ONNX, TensorFlow SavedModel).\n",
    "   - Package the serialized model along with any necessary dependencies and configurations.\n",
    "\n",
    "2. Choose a Deployment Architecture:\n",
    "   - Select an appropriate deployment architecture based on the requirements (e.g., RESTful API, microservices, serverless).\n",
    "   - Consider factors such as scalability, latency, and resource utilization.\n",
    "\n",
    "3. Containerize the Model:\n",
    "   - Create a container (e.g., Docker) that encapsulates the model and its dependencies.\n",
    "   - Configure the container to expose the necessary endpoints for model inference.\n",
    "\n",
    "4. Deploy the Model:\n",
    "   - Choose a suitable platform for deploying the containerized model (e.g., Kubernetes, AWS, GCP, Azure).\n",
    "   - Set up the necessary infrastructure and configurations for deployment.\n",
    "   - Deploy the model container to the chosen platform.\n",
    "5. Expose the Model Endpoint:\n",
    "   - Create an API endpoint that accepts input data and returns model predictions.\n",
    "   - Handle request/response formatting and any necessary data transformations.\n",
    "6. Monitor and Maintain:\n",
    "   - Implement monitoring and logging to track the model's performance and health.\n",
    "   - Set up alerts and notifications for any anomalies or errors.\n",
    "   - Regularly update and retrain the model as new data becomes available.\n",
    "   - Handle model versioning and deployment updates as needed.\n",
    "\n",
    "Here's the process expressed as a mermaid diagram:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Collect Data] --> B[Engineer Features]\n",
    "    B --> C[Train and Evaluate the Model]\n",
    "    C --> D[Evaluate Model]\n",
    "    D --> B\n",
    "    D --> E[Serialize and Package the Model]\n",
    "    D --> F[Choose a Deployment Architecture]\n",
    "    E --> G[Containerize the Model]\n",
    "    F --> G\n",
    "    G --> H[Deploy the Model]\n",
    "    H --> I[Expose the Model Endpoint]\n",
    "    I --> J[Monitor and Maintain]\n",
    "    J --> B\n",
    "```\n",
    "\n",
    "This diagram illustrates the high-level steps involved in the machine learning \n",
    "lifecycle, from training and evaluation to deployment, exposure, and maintenance. Each \n",
    "step plays a crucial role in ensuring the model is effectively served and can be \n",
    "accessed by the intended consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Serving Your First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first model, we wil use the Wine dataset from scikit-learn \n",
    "and serve it using FastAPI. The Wine dataset is a classic dataset from \n",
    "scikit-learn that contains information about different wine samples, \n",
    "including their chemical properties and the corresponding wine class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine(as_frame=True)\n",
    "wine.data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = wine.data.values, wine.target.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score(X_test, y_test)\n",
    "print(\"Model score on test set:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test[:3]), y_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, 'first_deployment/my_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile first_deployment/server.py\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class InputData(BaseModel):\n",
    "    alcohol: float\n",
    "    malic_acid: float\n",
    "    ash: float\n",
    "    alcalinity_of_ash: float\n",
    "    magnesium: float\n",
    "    total_phenols: float\n",
    "    flavanoids: float\n",
    "    nonflavanoid_phenols: float\n",
    "    proanthocyanins: float\n",
    "    color_intensity: float\n",
    "    hue: float\n",
    "    od280_od315_of_diluted_wines: float\n",
    "    proline: float\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    return joblib.load(\"my_model.joblib\")\n",
    "\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: InputData):\n",
    "    # Convert input data to a 2D array\n",
    "    features = [[\n",
    "        data.alcohol, data.malic_acid, data.ash, data.alcalinity_of_ash,\n",
    "        data.magnesium, data.total_phenols, data.flavanoids,\n",
    "        data.nonflavanoid_phenols, data.proanthocyanins, data.color_intensity,\n",
    "        data.hue, data.od280_od315_of_diluted_wines, data.proline\n",
    "    ]]\n",
    "    \n",
    "    # Make predictions using the loaded model\n",
    "    prediction = model.predict(features)\n",
    "    \n",
    "    # Return the predicted class\n",
    "    return {\"class\": prediction.tolist()}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You have successfully served a machine learning model using FastAPI with the Wine dataset. The API endpoint accepts input data, makes predictions using the trained random forest classifier, and returns the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://localhost:8000/predict\"\n",
    "data = {\n",
    "    \"alcohol\": 12.85,\n",
    "    \"malic_acid\": 1.6,\n",
    "    \"ash\": 2.52,\n",
    "    \"alcalinity_of_ash\": 17.8,\n",
    "    \"magnesium\": 95,\n",
    "    \"total_phenols\": 2.48,\n",
    "    \"flavanoids\": 2.37,\n",
    "    \"nonflavanoid_phenols\": 0.26,\n",
    "    \"proanthocyanins\": 1.46,\n",
    "    \"color_intensity\": 3.93,\n",
    "    \"hue\": 1.09,\n",
    "    \"od280_od315_of_diluted_wines\": 2.81,\n",
    "    \"proline\": 625\n",
    "}\n",
    "results = requests.post(endpoint, json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Intro to MLServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLServer is an open-source framework that simplifies the deployment of machine learning \n",
    "models as production-ready microservices. It provides a scalable and efficient solution \n",
    "for serving models, making it easier to integrate them into applications and workflows.\n",
    "\n",
    "MLServer offers several benefits, such as automatic API documentation, request validation, and support for various deployment scenarios, including containerization with Docker and orchestration with Kubernetes.\n",
    "\n",
    "By leveraging MLServer, we can easily serve our trained wine classification model as a scalable and production-ready microservice. This allows us to integrate the model into larger applications or workflows, enabling seamless predictions and decision-making based on the wine sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile second_deployment/model-settings.json\n",
    "{\n",
    "    \"name\": \"wine-classifier\",\n",
    "    \"implementation\": \"mlserver_sklearn.SKLearnModel\",\n",
    "    \"parameters\": {\n",
    "        \"uri\": \"../models/my_model.joblib\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlserver start second_deployment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out the docs of all of the methods at `http://0.0.0.0:8080/v2/docs`.\n",
    "\n",
    "![open api specs](\"./images/openapi.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_request = {\n",
    "    \"inputs\": [{\n",
    "        \"name\": \"my-input\",\n",
    "      \"datatype\": \"INT32\",\n",
    "      \"shape\": X_test[0, None].shape,\n",
    "      \"data\": X_test[0].tolist()\n",
    "    }]\n",
    "}\n",
    "input_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://0.0.0.0:8080/v2/models/wine-classifier/infer\"\n",
    "results = requests.post(endpoint, json=input_request)\n",
    "results.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california = fetch_california_housing(as_frame=True)\n",
    "california.data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = california.data.values, california.target.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score(X_test, y_test)\n",
    "print(\"Model score on test set:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, \"./models/california_housing_model.joblib\")\n",
    "print(\"Model saved as california_housing_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile third_deployment/model-settings.json\n",
    "{\n",
    "    \"name\": \"cali_model\",\n",
    "    \"implementation\": \"mlserver_sklearn.SKLearnModel\",\n",
    "    \"parameters\": {\n",
    "        \"uri\": \"../models/california_housing_model.joblib\",\n",
    "        \"version\": \"v0.1.0\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlserver start third_deployment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlserver.codecs import NumpyCodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumpyCodec.encode_input(name=\"predict\", payload=X_test[0, None]).dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_request = {\n",
    "    \"inputs\": [NumpyCodec.encode_input(name=\"predict\", payload=X_test[0, None]).dict()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://0.0.0.0:8080/v2/models/cali_model/infer\"\n",
    "results = requests.post(endpoint, json=input_request)\n",
    "results.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serving both models at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cali_model\"\n",
    "endpoint = f\"http://0.0.0.0:8080/v2/models/{model_name}/infer\"\n",
    "results = requests.post(endpoint, json=input_request)\n",
    "results.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine().data[0, None]\n",
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"wine-classifier\"\n",
    "input_request = {\n",
    "    \"inputs\": [NumpyCodec.encode_input(name=\"predict\", payload=wine).dict()]\n",
    "}\n",
    "endpoint = f\"http://0.0.0.0:8080/v2/models/{model_name}/infer\"\n",
    "results = requests.post(endpoint, json=input_request)\n",
    "results.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Serving Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"Qwen/Qwen1.5-0.5B-Chat-GGUF\",\n",
    "    filename=\"*q8_0.gguf\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who is an expert in geography and fun facts.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"What can you tell me about the capital of the Dominican Republic?\"\n",
    "          }\n",
    "      ]\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.create_chat_completion??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fourth_deployment/qwen_model.py\n",
    "from mlserver import MLModel\n",
    "from mlserver.codecs import decode_args\n",
    "from typing import List\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "class MyKulModel(MLModel):\n",
    "\n",
    "    async def load(self):\n",
    "        self.llm = Llama.from_pretrained(\n",
    "            repo_id=\"Qwen/Qwen1.5-0.5B-Chat-GGUF\",\n",
    "            filename=\"*q8_0.gguf\",\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    @decode_args\n",
    "    async def predict(self, system: List[str], user: List[str]) -> List[str]:\n",
    "\n",
    "        return [self.llm.create_chat_completion(\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system[0]},\n",
    "                {\"role\": \"user\", \"content\": user[0]}\n",
    "            ]\n",
    "        )['choices'][0]['message']['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fourth_deployment/model-settings.json\n",
    "{\n",
    "    \"name\": \"llama_qwen\",\n",
    "    \"implementation\": \"qwen_model.MyKulModel\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fourth_deployment/settings.json\n",
    "{\n",
    "    \"http_port\": 7070,\n",
    "    \"grpc_port\": 6070\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlserver.codecs import StringCodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama_qwen\"\n",
    "system_prompt = [\"You are a helpful assistant that is also an expert in data science.\"]\n",
    "user_prompt = [\"What is Analytics Vidhya and what do you know about it?\"]\n",
    "\n",
    "input_request = {\n",
    "    \"inputs\": [\n",
    "        StringCodec.encode_input(name=\"system\", payload=system_prompt, use_bytes=False).dict(),\n",
    "        StringCodec.encode_input(name=\"user\", payload=user_prompt, use_bytes=False).dict()\n",
    "    ]\n",
    "}\n",
    "endpoint = f\"http://0.0.0.0:7070/v2/models/{model_name}/infer\"\n",
    "results = requests.post(endpoint, json=input_request)\n",
    "results.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fourth_deployment/requirements.txt\n",
    "llama-cpp-python\n",
    "mlserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlserver build . -t ramonprz/mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm -p 7070:7070 ramonprz/mymodel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
